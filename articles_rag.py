import streamlit as st
import os
import pickle
import hashlib
import time
from datetime import datetime
from io import BytesIO

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ —É–¥–µ—Ä–∂–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ GPU
os.environ['OLLAMA_KEEP_ALIVE'] = '-1'  # –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –≤—ã–≥—Ä—É–∂–∞—Ç—å
os.environ['OLLAMA_NUM_PARALLEL'] = '1'  # –û–¥–Ω–∞ –º–æ–¥–µ–ª—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
os.environ['OLLAMA_MAX_LOADED_MODELS'] = '1'  # –ú–∞–∫—Å–∏–º—É–º 1 –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç–∏

from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
import tempfile
import json

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Å—Ç–∞—Ç–µ–π
ARTICLES_DB_DIR = "./articles_chroma_db"
ARTICLES_CACHE_DIR = "./articles_bm25_cache"
ARTICLES_METADATA_FILE = "./articles_metadata.json"

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
os.makedirs(ARTICLES_DB_DIR, exist_ok=True)
os.makedirs(ARTICLES_CACHE_DIR, exist_ok=True)

def russian_preprocess(text):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    import re
    text = re.sub(r'[^–∞-—è—ë\s]', ' ', text.lower())
    return [word for word in text.split() if len(word) > 2]

# –î–≤–∞ —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
articles_template = """
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–æ–º—É –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—É. 

–í–ê–ñ–ù–û: 
- –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ù–∏–∫–∞–∫–∏—Ö –∏–µ—Ä–æ–≥–ª–∏—Ñ–æ–≤.
- –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
- –°–æ–∑–¥–∞–≤–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ —Å –ø–æ–¥–ø—É–Ω–∫—Ç–∞–º–∏
- –î–µ–ª–∞–π –∫—Ä–∞—Å–∏–≤–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Å—Ç–∞—Ç–µ–π: {context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç –≤ —Ç–∞–∫–æ–º —Ñ–æ—Ä–º–∞—Ç–µ:

üß© **–û—Å–Ω–æ–≤–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã [—Ç–µ–º–∞ –≤–æ–ø—Ä–æ—Å–∞]**

1. **–ü–µ—Ä–≤—ã–π –∫–ª—é—á–µ–≤–æ–π –∞—Å–ø–µ–∫—Ç**
   - –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
   - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω—é–∞–Ω—Å—ã
   - –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

2. **–í—Ç–æ—Ä–æ–π –≤–∞–∂–Ω—ã–π –∞—Å–ø–µ–∫—Ç**  
   - –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
   - –í–ª–∏—è–Ω–∏–µ –Ω–∞ –±–∏–∑–Ω–µ—Å-–ø—Ä–æ—Ü–µ—Å—Å—ã
   - –°–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏

3. **–¢—Ä–µ—Ç–∏–π –∫—Ä–∏—Ç–∏—á–Ω—ã–π –º–æ–º–µ–Ω—Ç**
   - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏
   - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —ç—Ñ—Ñ–µ–∫—Ç—ã
   - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

‚úÖ **–ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã**

> –û—Å–Ω–æ–≤–Ω–æ–π –≤—ã–≤–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:** —á—Ç–æ —ç—Ç–æ –¥–∞–µ—Ç –±–∏–∑–Ω–µ—Å—É
- **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∏—Å–∫–∏:** —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –±–µ–∑ —ç—Ç–æ–≥–æ  
- **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:** –∫–∞–∫ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ

üìö **–ò—Å—Ç–æ—á–Ω–∏–∫–∏**

*–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤–∑—è—Ç–∞ –∏–∑ —Å—Ç–∞—Ç–µ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–æ–º—É –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—É*
"""

llm_template = """
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–æ–º—É –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—É. –í –±–∞–∑–µ —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

–í–ê–ñ–ù–û: 
- –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ù–∏–∫–∞–∫–∏—Ö –∏–µ—Ä–æ–≥–ª–∏—Ñ–æ–≤.
- –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ
- –ë—É–¥—å –∫—Ä–∞—Ç–æ–∫ –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–µ–Ω

–í–æ–ø—Ä–æ—Å: {question}

üß† **–û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –ø–æ —Ç–µ–º–µ**

**üéØ –û—Å–Ω–æ–≤–Ω–æ–µ:**
- –ö–ª—é—á–µ–≤–∞—è —Å—É—Ç—å –≤–æ–ø—Ä–æ—Å–∞
- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ —Ä–∏—Ç–µ–π–ª–µ

**üíº –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–æ–º –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–µ:**
- –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ
- –°–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏

> ‚ö†Ô∏è *–û—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏—è—Ö, –Ω–µ –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏*
"""

def get_available_models():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama"""
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            models = response.json().get("models", [])
            return [model["name"] for model in models]
    except:
        pass
    return ["qwen2.5:14b", "deepseek-r1:14b", "qwen3:8b", "llama3:8b", "gemma3:4b"]

def unload_model(model_name):
    """–í—ã–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏ GPU"""
    try:
        import requests
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={"model": model_name, "keep_alive": 0}
        )
        return response.status_code == 200
    except:
        return False

def get_loaded_models():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç—å –º–æ–¥–µ–ª–µ–π"""
    try:
        import requests
        response = requests.get("http://localhost:11434/api/ps")
        if response.status_code == 200:
            models = response.json().get("models", [])
            result = []
            for model in models:
                name = model["name"]
                size_bytes = model.get("size", 0)
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –±–∞–π—Ç—ã –≤ GB
                size_gb = size_bytes / (1024**3)
                result.append((name, size_gb))
            return result
    except:
        pass
    return []

def create_model(model_name):
    """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º –∏–º–µ–Ω–µ–º –∏ –≤—ã–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â—É—é"""
    # –í—ã–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ–Ω–∞ –±—ã–ª–∞
    if 'current_model' in st.session_state and st.session_state.current_model != model_name:
        old_model = st.session_state.current_model
        if unload_model(old_model):
            st.sidebar.info(f"üóëÔ∏è –í—ã–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å {old_model}")
    
    return OllamaLLM(
        model=model_name,
        keep_alive=-1,  # –î–µ—Ä–∂–∏–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
        temperature=0.1
    )

model = OllamaLLM(
    model="qwen2.5:14b",
    keep_alive=-1,  # –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –≤—ã–≥—Ä—É–∂–∞—Ç—å –∏–∑ –ø–∞–º—è—Ç–∏
    temperature=0.1  # –ë–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
)

def extract_title(text):
    """–ü—ã—Ç–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏ –∏–∑ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫"""
    lines = text.split('\n')[:10]
    for line in lines:
        cleaned = line.strip()
        if 20 <= len(cleaned) <= 150:  # –†–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            return cleaned
    return "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

def load_articles_metadata():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Å—Ç–∞—Ç–µ–π"""
    try:
        with open(ARTICLES_METADATA_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {
            "articles": [],
            "total_fragments": 0,
            "total_size_mb": 0,
            "last_updated": None,
            "collection_initialized": False
        }

def save_articles_metadata(metadata):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Å—Ç–∞—Ç–µ–π"""
    metadata["last_updated"] = datetime.now().isoformat()
    with open(ARTICLES_METADATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

def process_articles_batch(uploaded_files):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–∞–∫–µ—Ç —Å—Ç–∞—Ç–µ–π"""
    all_documents = []
    articles_info = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, uploaded_file in enumerate(uploaded_files):
        status_text.text(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é: {uploaded_file.name}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º PDF
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            temp_file.write(uploaded_file.getvalue())
            temp_file_path = temp_file.name
        
        try:
            loader = PDFPlumberLoader(temp_file_path)
            documents = loader.load()
            
            if not documents:
                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å {uploaded_file.name}")
                continue
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è —Å—Ç–∞—Ç–µ–π)
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1500,  # –ë–æ–ª—å—à–µ –¥–ª—è —Å—Ç–∞—Ç–µ–π
                chunk_overlap=300,
                add_start_index=True
            )
            chunks = text_splitter.split_documents(documents)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏
            full_text = "\n".join([doc.page_content for doc in documents])
            article_title = extract_title(full_text)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ –∫–∞–∂–¥–æ–º—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç—É
            for chunk in chunks:
                chunk.metadata.update({
                    "source_file": uploaded_file.name,
                    "article_title": article_title,
                    "upload_date": datetime.now().isoformat(),
                    "file_size_mb": round(uploaded_file.size / (1024*1024), 2)
                })
            
            all_documents.extend(chunks)
            articles_info.append({
                "filename": uploaded_file.name,
                "title": article_title,
                "fragments": len(chunks),
                "size_mb": round(uploaded_file.size / (1024*1024), 2)
            })
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {uploaded_file.name}: {str(e)}")
        finally:
            os.unlink(temp_file_path)
        
        progress_bar.progress((i + 1) / len(uploaded_files))
    
    status_text.text("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    return all_documents, articles_info

def get_embeddings(model_name):
    """–°–æ–∑–¥–∞–µ—Ç embedding —Ñ—É–Ω–∫—Ü–∏—é —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
    return OllamaEmbeddings(model=model_name)

def initialize_articles_retriever():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ—Ç—Ä–∏–≤–µ—Ä –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π –ë–ï–ó –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
    # BM25 —Ä–µ—Ç—Ä–∏–≤–µ—Ä (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç embeddings)
    bm25_file = os.path.join(ARTICLES_CACHE_DIR, "articles_bm25.pkl")
    
    if os.path.exists(bm25_file):
        with open(bm25_file, 'rb') as f:
            bm25_data = pickle.load(f)
        bm25_retriever = BM25Retriever(
            vectorizer=bm25_data['bm25'],
            docs=bm25_data['docs'],
            k=8,
            preprocess_func=russian_preprocess
        )
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ BM25 - –±–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        return bm25_retriever
    else:
        return None

def create_hybrid_retriever(embedding_model_name):
    """–°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä —Å embeddings –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –ø–æ–∏—Å–∫–µ"""
    # –°–æ–∑–¥–∞–µ–º embeddings —Ç–æ–ª—å–∫–æ —Å–µ–π—á–∞—Å
    embeddings = OllamaEmbeddings(model=embedding_model_name)
    
    # –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    vector_store = Chroma(
        collection_name="category_management_articles",
        embedding_function=embeddings,
        persist_directory=ARTICLES_DB_DIR
    )
    
    semantic_retriever = vector_store.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 8, "fetch_k": 16, "lambda_mult": 0.8}
    )
    
    # BM25 –ø–æ–∏—Å–∫
    bm25_file = os.path.join(ARTICLES_CACHE_DIR, "articles_bm25.pkl")
    if os.path.exists(bm25_file):
        with open(bm25_file, 'rb') as f:
            bm25_data = pickle.load(f)
        bm25_retriever = BM25Retriever(
            vectorizer=bm25_data['bm25'],
            docs=bm25_data['docs'],
            k=8,
            preprocess_func=russian_preprocess
        )
        
        # –ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä
        return EnsembleRetriever(
            retrievers=[semantic_retriever, bm25_retriever],
            weights=[0.6, 0.4]
        )
    else:
        return semantic_retriever

def add_articles_to_retriever(documents, embedding_model):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ - –ó–î–ï–°–¨ —Å–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
    embeddings = get_embeddings(embedding_model)  # ‚Üê –¢–æ–ª—å–∫–æ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏!
    
    vector_store = Chroma(
        collection_name="category_management_articles",
        embedding_function=embeddings,
        persist_directory=ARTICLES_DB_DIR
    )
    vector_store.add_documents(documents)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º BM25 –∏–Ω–¥–µ–∫—Å
    bm25_file = os.path.join(ARTICLES_CACHE_DIR, "articles_bm25.pkl")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    existing_docs = []
    if os.path.exists(bm25_file):
        with open(bm25_file, 'rb') as f:
            existing_data = pickle.load(f)
            existing_docs = existing_data.get('docs', [])
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –Ω–æ–≤—ã–º–∏
    all_docs = existing_docs + documents
    bm25_retriever = BM25Retriever.from_documents(
        all_docs, 
        preprocess_func=russian_preprocess,
        k=8
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
    bm25_data = {
        'docs': all_docs,
        'bm25': bm25_retriever.vectorizer,
        'k': 8
    }
    with open(bm25_file, 'wb') as f:
        pickle.dump(bm25_data, f)
    
    return initialize_articles_retriever()

def has_relevant_content(documents, threshold=0.5):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –µ—Å—Ç—å –ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"""
    if not documents or len(documents) < 2:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â—É—é –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    total_length = sum(len(doc.page_content) for doc in documents[:5])
    return total_length > 2000  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞

def answer_from_articles(question, documents):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–µ–π"""
    start_time = time.time()
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å—Ç–∞—Ç—å—è–º –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    articles_content = {}
    for doc in documents[:6]:  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 12 –¥–æ 6 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        article_title = doc.metadata.get('article_title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞')
        source_file = doc.metadata.get('source_file', '')
        
        key = f"{article_title} ({source_file})"
        if key not in articles_content:
            articles_content[key] = []
        articles_content[key].append(doc.page_content)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    context_parts = []
    for article_key, contents in articles_content.items():
        combined_content = "\n".join(contents[:2])  # –ú–∞–∫—Å–∏–º—É–º 2 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –Ω–∞ —Å—Ç–∞—Ç—å—é
        context_parts.append(f"–ò–∑ —Å—Ç–∞—Ç—å–∏ '{article_key}':\n{combined_content}")
    
    context = "\n\n".join(context_parts)
    
    # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    generation_start = time.time()
    prompt = ChatPromptTemplate.from_template(articles_template)
    chain = prompt | model
    result = chain.invoke({"question": question, "context": context})
    generation_time = time.time() - generation_start
    
    total_time = time.time() - start_time
    
    return {
        "answer": result,
        "generation_time": generation_time,
        "total_time": total_time,
        "fragments_used": len(documents[:6]),
        "sources_count": len(articles_content)
    }

def answer_from_llm(question):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–Ω–∞–Ω–∏–π LLM"""
    start_time = time.time()
    
    prompt = ChatPromptTemplate.from_template(llm_template)
    chain = prompt | model
    result = chain.invoke({"question": question})
    
    total_time = time.time() - start_time
    
    return {
        "answer": result,
        "generation_time": total_time,
        "total_time": total_time
    }

def clean_response(response):
    """–û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    import re
    
    # –£–±–∏—Ä–∞–µ–º –∞–∑–∏–∞—Ç—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
    cleaned = re.sub(r'[\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff\uac00-\ud7af]+', '', response)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã ###
    cleaned = re.sub(r'#{3,}', '', cleaned)
    
    # –£–±–∏—Ä–∞–µ–º –¢–û–õ–¨–ö–û –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–µ—Ä–µ–Ω–æ—Å—ã
    cleaned = re.sub(r'[ \t]+', ' ', cleaned)  # –ó–∞–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã –∏ —Ç–∞–±—ã
    cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned)  # –£–±–∏—Ä–∞–µ–º –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    
    # –£–±–∏—Ä–∞–µ–º –≤–∏—Å—è—â–∏–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    cleaned = re.sub(r'[,Ôºå„ÄÇÔºö:]+\s*$', '', cleaned)
    cleaned = re.sub(r'^\s*[,Ôºå„ÄÇÔºö:]+', '', cleaned)
    
    return cleaned.strip()

def is_simple_list_question(question):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø—Ä–æ —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    simple_keywords = ["–∫–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã", "—Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π", "—á—Ç–æ –µ—Å—Ç—å –≤ –±–∞–∑–µ", "–∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã", "–¥–æ–∫—É–º–µ–Ω—Ç—ã –µ—Å—Ç—å"]
    return any(keyword in question.lower() for keyword in simple_keywords)

def get_articles_list():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
    metadata = load_articles_metadata()
    if not metadata["articles"]:
        return "–í –±–∏–±–ª–∏–æ—Ç–µ–∫–µ –ø–æ–∫–∞ –Ω–µ—Ç —Å—Ç–∞—Ç–µ–π."
    
    result = f"üìö –í –±–∏–±–ª–∏–æ—Ç–µ–∫–µ {len(metadata['articles'])} —Å—Ç–∞—Ç–µ–π:\n\n"
    for i, article in enumerate(metadata["articles"], 1):
        result += f"{i}. **{article['title']}**\n   üìÑ {article['filename']} ({article['fragments']} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤)\n\n"
    
    return result

def display_answer_beautifully(answer_text, result_info, search_time=None):
    """–ö—Ä–∞—Å–∏–≤–æ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –æ—Ç–≤–µ—Ç —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç
    st.markdown(answer_text)
    
    # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å  
    st.divider()
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if search_time:
            st.metric("üîç –ü–æ–∏—Å–∫", f"{search_time:.2f}—Å")
        else:
            st.metric("üß† –†–µ–∂–∏–º", "LLM")
            
    with col2:
        st.metric("‚è±Ô∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è", f"{result_info['generation_time']:.2f}—Å")
        
    with col3:
        if 'fragments_used' in result_info:
            st.metric("üìä –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤", result_info['fragments_used'])
        else:
            st.metric("üìö –ò—Å—Ç–æ—á–Ω–∏–∫", "–û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è")
            
    with col4:
        if 'sources_count' in result_info:
            st.metric("üìë –°—Ç–∞—Ç–µ–π", result_info['sources_count'])
        else:
            st.metric("üéØ –ö–∞—á–µ—Å—Ç–≤–æ", "–í—ã—Å–æ–∫–æ–µ")

def display_sources_beautifully(docs, max_sources):
    """–ö—Ä–∞—Å–∏–≤–æ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    with st.expander("üìã **–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏**", expanded=False):
        sources = {}
        for doc in docs[:max_sources]:
            article = doc.metadata.get('article_title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞')
            file = doc.metadata.get('source_file', '')
            if article not in sources:
                sources[article] = {
                    'file': file,
                    'fragments': 0
                }
            sources[article]['fragments'] += 1
        
        st.markdown("#### üìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏:")
        for i, (article, info) in enumerate(sources.items(), 1):
            st.markdown(f"""
            **{i}. {article}**  
            üìÑ *{info['file']}*  
            üî¢ *{info['fragments']} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ*
            """)
            if i < len(sources):
                st.divider()
        
        st.info(f"üìä **–ò—Ç–æ–≥–æ:** {len(docs)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ {len(sources)} —Å—Ç–∞—Ç–µ–π")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å—Ç–∞—Ç–µ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–æ–º—É –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—É", 
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("üìö –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å—Ç–∞—Ç–µ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–æ–º—É –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—É")
st.markdown("*–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Å—Ç–∞—Ç–µ–π —Å fallback –Ω–∞ –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è*")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session state
if 'articles_retriever' not in st.session_state:
    st.session_state.articles_retriever = None
if 'library_loaded' not in st.session_state:
    st.session_state.library_loaded = False

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
articles_metadata = load_articles_metadata()

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
if not st.session_state.library_loaded and articles_metadata["articles"]:
    with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É —Å—Ç–∞—Ç–µ–π..."):
        try:
            st.session_state.articles_retriever = initialize_articles_retriever()
            st.session_state.library_loaded = True
            st.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞: {len(articles_metadata['articles'])} —Å—Ç–∞—Ç–µ–π")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: {str(e)}")

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
st.sidebar.title("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏")

if articles_metadata["articles"]:
    col1, col2 = st.sidebar.columns(2)
    with col1:
        st.metric("–°—Ç–∞—Ç–µ–π", len(articles_metadata['articles']))
        st.metric("–§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤", articles_metadata['total_fragments'])
    with col2:
        st.metric("–†–∞–∑–º–µ—Ä", f"{articles_metadata['total_size_mb']:.1f} –ú–ë")
        last_update = articles_metadata.get('last_updated', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        if last_update != '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ':
            try:
                update_date = datetime.fromisoformat(last_update.replace('Z', '+00:00'))
                st.metric("–û–±–Ω–æ–≤–ª–µ–Ω–æ", update_date.strftime('%d.%m.%Y'))
            except:
                st.metric("–û–±–Ω–æ–≤–ª–µ–Ω–æ", "–ù–µ–¥–∞–≤–Ω–æ")
    
    with st.sidebar.expander("üìë –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π"):
        for i, article in enumerate(articles_metadata["articles"], 1):
            st.write(f"**{i}. {article['title']}**")
            st.caption(f"üìÑ {article['filename']}")
            st.caption(f"üíæ {article['size_mb']} –ú–ë, {article['fragments']} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            st.divider()
else:
    st.sidebar.info("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—É—Å—Ç–∞. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å—Ç–∞—Ç—å–∏ –Ω–∏–∂–µ.")

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å - –ù–∞—Å—Ç—Ä–æ–π–∫–∏
st.sidebar.title("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")

search_strategy = st.sidebar.selectbox(
    "–°—Ç—Ä–∞—Ç–µ–≥–∏—è –æ—Ç–≤–µ—Ç–∞:",
    ["üîÑ –ê–≤—Ç–æ (—Å—Ç–∞—Ç—å–∏ ‚Üí LLM)", "üìö –¢–æ–ª—å–∫–æ —Å—Ç–∞—Ç—å–∏", "üß† –¢–æ–ª—å–∫–æ LLM"],
    help="–ê–≤—Ç–æ: —Å–Ω–∞—á–∞–ª–∞ –∏—â–µ—Ç –≤ —Å—Ç–∞—Ç—å—è—Ö, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è"
)

relevance_threshold = st.sidebar.slider(
    "–ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏", 
    0.1, 1.0, 0.5, 0.1,
    help="–ù–∞—Å–∫–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"
)

max_articles_sources = st.sidebar.slider(
    "–ú–∞–∫—Å. –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ", 
    1, 10, 5,
    help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –æ—Ç–≤–µ—Ç–µ"
)

# –í –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ–∏—Å–∫–∞ –¥–æ–±–∞–≤–ª—è–µ–º:
st.sidebar.title("ü§ñ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏")

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
loaded_models = get_loaded_models()
if loaded_models:
    st.sidebar.write("**üíæ –í –ø–∞–º—è—Ç–∏ GPU:**")
    total_size = 0
    for name, size_gb in loaded_models:
        total_size += size_gb
        st.sidebar.write(f"‚Ä¢ {name}: {size_gb:.1f} GB")
    st.sidebar.write(f"**–í—Å–µ–≥–æ: {total_size:.1f} GB**")
    
    # –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    if st.sidebar.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –ø–∞–º—è—Ç—å GPU"):
        for name, _ in loaded_models:
            unload_model(name)
        st.sidebar.success("‚úÖ –ü–∞–º—è—Ç—å GPU –æ—á–∏—â–µ–Ω–∞")
        st.rerun()

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö
model_info = {
    "qwen2.5:14b": "üß† –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è (9GB) - —Ö–æ—Ä–æ—à–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á",
    "deepseek-r1:14b": "üéØ –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ (9GB) - –æ—Ç–ª–∏—á–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ", 
    "qwen3:8b": "‚ö° –ë—ã—Å—Ç—Ä–∞—è (5GB) - —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞",
    "llama3:8b": "üåü –°—Ç–∞–±–∏–ª—å–Ω–∞—è (5GB) - –Ω–∞–¥–µ–∂–Ω—ã–π —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫",
    "gemma3:4b": "üöÄ –°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è (3GB) - —ç–∫–æ–Ω–æ–º–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤"
}

# –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
available_models = get_available_models()
selected_model = st.sidebar.selectbox(
    "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:",
    available_models,
    index=0 if "qwen2.5:14b" in available_models else 0,
    format_func=lambda x: f"{x} - {model_info.get(x, '–ú–æ–¥–µ–ª—å Ollama')}"
)

# –°–æ–∑–¥–∞–µ–º/–º–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏
if 'current_model' not in st.session_state or st.session_state.current_model != selected_model:
    with st.spinner(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞—é –Ω–∞ –º–æ–¥–µ–ª—å {selected_model}..."):
        model = create_model(selected_model)
        st.session_state.current_model = selected_model
    st.sidebar.success(f"‚úÖ –ê–∫—Ç–∏–≤–Ω–∞ –º–æ–¥–µ–ª—å {selected_model}")
else:
    model = create_model(selected_model)

# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å - –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
st.header("üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç–µ–π")

uploaded_files = st.file_uploader(
    "–í—ã–±–µ—Ä–∏—Ç–µ PDF —Ñ–∞–π–ª—ã —Å—Ç–∞—Ç–µ–π",
    type="pdf",
    accept_multiple_files=True,
    help=f"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ 20 PDF —Ñ–∞–π–ª–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. –¢–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: {articles_metadata.get('total_size_mb', 0):.1f} –ú–ë"
)

if uploaded_files:
    st.info(f"–í—ã–±—Ä–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(uploaded_files)} (–æ–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {sum(f.size for f in uploaded_files) / (1024*1024):.1f} –ú–ë)")
    
    if len(uploaded_files) > 20:
        st.warning("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–≥—Ä—É–∂–∞—Ç—å –Ω–µ –±–æ–ª–µ–µ 20 —Ñ–∞–π–ª–æ–≤ –∑–∞ —Ä–∞–∑ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã.")
    
    if st.button("üîÑ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏ –¥–æ–±–∞–≤–∏—Ç—å –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É", type="primary"):
        with st.spinner("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å—Ç–∞—Ç—å–∏..."):
            try:
                documents, articles_info = process_articles_batch(uploaded_files)
                
                if documents:
                    st.success(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(uploaded_files)} —Å—Ç–∞—Ç–µ–π, —Å–æ–∑–¥–∞–Ω–æ {len(documents)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ—Ç—Ä–∏–≤–µ—Ä
                    if st.session_state.articles_retriever is None:
                        st.session_state.articles_retriever = add_articles_to_retriever(documents, selected_model)
                    else:
                        st.session_state.articles_retriever = add_articles_to_retriever(documents, selected_model)
                    
                    st.session_state.library_loaded = True
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    articles_metadata["articles"].extend(articles_info)
                    articles_metadata["total_fragments"] += len(documents)
                    articles_metadata["total_size_mb"] += sum(info["size_mb"] for info in articles_info)
                    articles_metadata["collection_initialized"] = True
                    save_articles_metadata(articles_metadata)
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏
                    with st.expander("üìã –î–µ—Ç–∞–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"):
                        for info in articles_info:
                            st.write(f"‚Ä¢ **{info['title']}** - {info['fragments']} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ ({info['size_mb']} –ú–ë)")
                    
                    st.rerun()
                else:
                    st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
                    
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}")

# –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å - –ß–∞—Ç
st.header("üí¨ –ß–∞—Ç —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π")

if st.session_state.articles_retriever or search_strategy == "üß† –¢–æ–ª—å–∫–æ LLM":
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å –≤ –∫—Ä–∞—Å–∏–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    if search_strategy == "üß† –¢–æ–ª—å–∫–æ LLM":
        st.info("üß† **–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã:** –û—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π –º–æ–¥–µ–ª–∏")
    elif articles_metadata["articles"]:
        st.success(f"üìö **–ì–æ—Ç–æ–≤ –∫ –ø–æ–∏—Å–∫—É!** {len(articles_metadata['articles'])} —Å—Ç–∞—Ç–µ–π ‚Ä¢ {articles_metadata['total_fragments']} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ ‚Ä¢ {articles_metadata['total_size_mb']:.1f} –ú–ë –¥–∞–Ω–Ω—ã—Ö")
    else:
        st.warning("üí≠ **–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø—É—Å—Ç–∞** ‚Äî –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è")
    
    question = st.chat_input("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–æ–º—É –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—É...")
    
    if question:
        with st.chat_message("user"):
            st.write(question)
        
        with st.chat_message("assistant"):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Å–Ω–∞—á–∞–ª–∞
            if is_simple_list_question(question):
                articles_list = get_articles_list()
                st.write(articles_list)
                st.caption("üìã –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏")
                
            elif search_strategy == "üß† –¢–æ–ª—å–∫–æ LLM":
                # –¢–æ–ª—å–∫–æ LLM
                with st.spinner("üí≠ –û—Ç–≤–µ—á–∞—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π..."):
                    result = answer_from_llm(question)
                    answer = clean_response(result["answer"])
                    display_answer_beautifully(answer, result)
                    
            elif search_strategy == "üìö –¢–æ–ª—å–∫–æ —Å—Ç–∞—Ç—å–∏":
                # –¢–æ–ª—å–∫–æ —Å—Ç–∞—Ç—å–∏
                if st.session_state.articles_retriever:
                    with st.spinner("üîç –ò—â—É –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ —Å—Ç–∞—Ç–µ–π..."):
                        search_start = time.time()
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ —Å–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä
                        if isinstance(st.session_state.articles_retriever, BM25Retriever):
                            # –ü–µ—Ä–≤—ã–π –ø–æ–∏—Å–∫ - —Å–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä —Å embeddings
                            with st.spinner("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫..."):
                                st.session_state.articles_retriever = create_hybrid_retriever(selected_model)
                        
                        docs = st.session_state.articles_retriever.invoke(question)
                        search_time = time.time() - search_start
                        
                        if docs and has_relevant_content(docs, relevance_threshold):
                            result = answer_from_articles(question, docs)
                            answer = clean_response(result["answer"])
                            display_answer_beautifully(answer, result, search_time)
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                            display_sources_beautifully(docs, max_articles_sources)
                        else:
                            st.warning("üì≠ –í –±–∏–±–ª–∏–æ—Ç–µ–∫–µ —Å—Ç–∞—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É.")
                            st.caption(f"üîç –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {search_time:.2f}—Å")
                else:
                    st.error("‚ùå **–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å—Ç–∞—Ç–µ–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞**  \n–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å—Ç–∞—Ç—å–∏ —Å–Ω–∞—á–∞–ª–∞.")
                    
            else:  # –ê–≤—Ç–æ —Ä–µ–∂–∏–º
                if st.session_state.articles_retriever:
                    with st.spinner("üîç –ò—â—É –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ —Å—Ç–∞—Ç–µ–π..."):
                        search_start = time.time()
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ —Å–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä
                        if isinstance(st.session_state.articles_retriever, BM25Retriever):
                            # –ü–µ—Ä–≤—ã–π –ø–æ–∏—Å–∫ - —Å–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä —Å embeddings
                            with st.spinner("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫..."):
                                st.session_state.articles_retriever = create_hybrid_retriever(selected_model)
                        
                        docs = st.session_state.articles_retriever.invoke(question)
                        search_time = time.time() - search_start
                        
                        if has_relevant_content(docs, relevance_threshold):
                            # –ù–∞–π–¥–µ–Ω–æ –≤ —Å—Ç–∞—Ç—å—è—Ö
                            result = answer_from_articles(question, docs)
                            answer = clean_response(result["answer"])
                            display_answer_beautifully(answer, result, search_time)
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                            display_sources_beautifully(docs, max_articles_sources)
                        else:
                            # Fallback –Ω–∞ LLM
                            st.info("üì≠ **–í —Å—Ç–∞—Ç—å—è—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏**  \nüîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è")
                            st.caption(f"üîç –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {search_time:.2f}—Å")
                            
                            with st.spinner("üí≠ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π..."):
                                result = answer_from_llm(question)
                                answer = clean_response(result["answer"])
                                display_answer_beautifully(answer, result)
                else:
                    # –ù–µ—Ç —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM
                    st.warning("üìö **–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞**  \nüß† –ò—Å–ø–æ–ª—å–∑—É—é –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è")
                    with st.spinner("üí≠ –û—Ç–≤–µ—á–∞—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π..."):
                        result = answer_from_llm(question)
                        answer = clean_response(result["answer"])
                        display_answer_beautifully(answer, result)
else:
    st.info("üëÜ –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Å—Ç–∞—Ç—å–∏ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É, –∏–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º '–¢–æ–ª—å–∫–æ LLM'")

# –§—É—Ç–µ—Ä
st.markdown("---")
st.markdown("*–°–∏—Å—Ç–µ–º–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ —Å—Ç–∞—Ç—å—è–º –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞*") 