import streamlit as st
import os
import pickle
import hashlib
from io import BytesIO

from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
import tempfile

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
CHROMA_DB_DIR = "./chroma_db"
BM25_CACHE_DIR = "./bm25_cache"
DOCS_METADATA_FILE = "./processed_docs.json"

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
os.makedirs(CHROMA_DB_DIR, exist_ok=True)
os.makedirs(BM25_CACHE_DIR, exist_ok=True)

def russian_preprocess(text):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    import re
    text = re.sub(r'[^–∞-—è—ë\s]', ' ', text.lower())
    return [word for word in text.split() if len(word) > 2]

template = """
–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É. –ò—Å–ø–æ–ª—å–∑—É–π –≤–µ—Å—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∏ —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.

–í–ê–ñ–ù–û: –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ù–∏–∫–∞–∫–∏—Ö –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏–ª–∏ –∏–µ—Ä–æ–≥–ª–∏—Ñ–æ–≤.

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ü–µ–ª–∏–∫–æ–º. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –æ–±—â–µ–≥–æ –æ–±–∑–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ - –¥–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç.
–ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.

–í–æ–ø—Ä–æ—Å: {question} 
–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context} 

–î–∞–π –ø–æ–ª–Ω—ã–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:
"""

model = OllamaLLM(model="qwen2.5:14b")

def get_file_hash(uploaded_file):
    """–°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ö–µ—à –¥–ª—è —Ñ–∞–π–ª–∞"""
    file_content = uploaded_file.getvalue()
    return hashlib.md5(file_content).hexdigest()

def load_docs_metadata():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    import json
    try:
        with open(DOCS_METADATA_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {}

def save_docs_metadata(metadata):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    import json
    with open(DOCS_METADATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

def load_pdf_from_uploaded_file(uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
        temp_file.write(uploaded_file.getvalue())
        temp_file_path = temp_file.name
    
    try:
        loader = PDFPlumberLoader(temp_file_path)
        documents = loader.load()
        return documents
    finally:
        os.unlink(temp_file_path)

def split_text(documents):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,
        chunk_overlap=400,
        add_start_index=True
    )
    return text_splitter.split_documents(documents)

def build_semantic_retriever(documents, collection_name):
    """–°–æ–∑–¥–∞–µ—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å"""
    embeddings = OllamaEmbeddings(model="qwen2.5:14b")
    
    vector_store = Chroma(
        collection_name=collection_name,
        embedding_function=embeddings,
        persist_directory=CHROMA_DB_DIR
    )
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞—è
    if vector_store._collection.count() == 0:
        vector_store.add_documents(documents)
    
    return vector_store.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 10,
            "fetch_k": 20,
            "lambda_mult": 0.7
        }
    )

def build_bm25_retriever(documents, file_hash):
    """–°–æ–∑–¥–∞–µ—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç BM25 –∏–Ω–¥–µ–∫—Å"""
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ—Ç–¥–µ–ª—å–Ω–æ
    if documents:  # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        retriever = BM25Retriever.from_documents(
            documents, 
            preprocess_func=russian_preprocess,
            k=10
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞
        bm25_data = {
            'docs': retriever.docs,
            'bm25': retriever.vectorizer,
            'k': getattr(retriever, 'k', 10),
            'preprocess_func_name': 'russian_preprocess'
        }
        
        bm25_file = os.path.join(BM25_CACHE_DIR, f"{file_hash}_bm25.pkl")
        with open(bm25_file, 'wb') as f:
            pickle.dump(bm25_data, f)
        
        return retriever
    else:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        bm25_file = os.path.join(BM25_CACHE_DIR, f"{file_hash}_bm25.pkl")
        
        if os.path.exists(bm25_file):
            with open(bm25_file, 'rb') as f:
                bm25_data = pickle.load(f)
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ—Ç—Ä–∏–≤–µ—Ä –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            retriever = BM25Retriever(
                vectorizer=bm25_data['bm25'],
                docs=bm25_data['docs'],
                k=bm25_data.get('k', 10),
                preprocess_func=russian_preprocess
            )
            return retriever
        else:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä
            return BM25Retriever.from_documents([], preprocess_func=russian_preprocess, k=10)

def clean_response(response):
    import re
    cleaned = re.sub(r'[\u4e00-\u9fff]+', '', response)
    return cleaned.strip()

def answer_question(question, documents):
    context = "\n\n".join([doc.page_content for doc in documents])
    
    system_message = """–¢—ã —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. 
    –ó–∞–ø—Ä–µ—â–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã –∏–ª–∏ –ª—é–±—ã–µ –∏–µ—Ä–æ–≥–ª–∏—Ñ—ã."""
    
    user_message = f"""
    –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}
    
    –í–æ–ø—Ä–æ—Å: {question}
    
    –î–∞–π –∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ (–º–∞–∫—Å–∏–º—É–º 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è):
    """
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_message),
        ("user", user_message)
    ])
    
    chain = prompt | model
    result = chain.invoke({})
    return clean_response(result)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º session state
if 'current_doc_hash' not in st.session_state:
    st.session_state.current_doc_hash = None
if 'retriever' not in st.session_state:
    st.session_state.retriever = None

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
docs_metadata = load_docs_metadata()

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
st.sidebar.title("üìö –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞
st.sidebar.title("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
search_fragments = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤", 5, 20, 10)
chunk_info = st.sidebar.info(f"–†–∞–∑–º–µ—Ä —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞: 2000 —Å–∏–º–≤–æ–ª–æ–≤\n–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: 400 —Å–∏–º–≤–æ–ª–æ–≤")

if docs_metadata:
    selected_doc = st.sidebar.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç:",
        options=[""] + list(docs_metadata.keys()),
        format_func=lambda x: "–í—ã–±–µ—Ä–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç..." if x == "" else f"{docs_metadata[x]['name']} ({docs_metadata[x]['fragments']} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤)"
    )
    
    if selected_doc and selected_doc != st.session_state.current_doc_hash:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        st.session_state.current_doc_hash = selected_doc
        
        with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç..."):
            semantic_retriever = build_semantic_retriever([], selected_doc)
            bm25_retriever = build_bm25_retriever([], selected_doc)
            
            st.session_state.retriever = EnsembleRetriever(
                retrievers=[semantic_retriever, bm25_retriever],
                weights=[0.5, 0.5]
            )
        
        st.success(f"–î–æ–∫—É–º–µ–Ω—Ç '{docs_metadata[selected_doc]['name']}' –∑–∞–≥—Ä—É–∂–µ–Ω!")
else:
    st.sidebar.write("–ü–æ–∫–∞ –Ω–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
uploaded_file = st.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—ã–π PDF",
    type="pdf",
    accept_multiple_files=False
)

if uploaded_file:
    file_hash = get_file_hash(uploaded_file)
    
    if file_hash in docs_metadata:
        st.info(f"üìã –î–æ–∫—É–º–µ–Ω—Ç '{uploaded_file.name}' —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω!")
        if st.button("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"):
            st.session_state.current_doc_hash = file_hash
            
            semantic_retriever = build_semantic_retriever([], file_hash)
            bm25_retriever = build_bm25_retriever([], file_hash)
            
            st.session_state.retriever = EnsembleRetriever(
                retrievers=[semantic_retriever, bm25_retriever],
                weights=[0.5, 0.5]
            )
            
            st.success("–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω!")
    else:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        st.info(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –Ω–æ–≤—ã–π —Ñ–∞–π–ª —Ä–∞–∑–º–µ—Ä–æ–º {uploaded_file.size / (1024*1024):.1f} –ú–ë...")
        
        with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –¥–æ–∫—É–º–µ–Ω—Ç..."):
            documents = load_pdf_from_uploaded_file(uploaded_file)
            chunked_documents = split_text(documents)
            
            st.info(f"–°–æ–∑–¥–∞–Ω–æ {len(chunked_documents)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            
            semantic_retriever = build_semantic_retriever(chunked_documents, file_hash)
            bm25_retriever = build_bm25_retriever(chunked_documents, file_hash)
            
            st.session_state.retriever = EnsembleRetriever(
                retrievers=[semantic_retriever, bm25_retriever],
                weights=[0.5, 0.5]
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            docs_metadata[file_hash] = {
                'name': uploaded_file.name,
                'fragments': len(chunked_documents),
                'size_mb': round(uploaded_file.size / (1024*1024), 1)
            }
            save_docs_metadata(docs_metadata)
            
            st.session_state.current_doc_hash = file_hash
            
        st.success("‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")
        st.rerun()

# –ß–∞—Ç –¥–æ—Å—Ç—É–ø–µ–Ω —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω
if st.session_state.retriever and st.session_state.current_doc_hash:
    current_doc_name = docs_metadata.get(st.session_state.current_doc_hash, {}).get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç')
    fragments_count = docs_metadata.get(st.session_state.current_doc_hash, {}).get('fragments', 0)
    
    st.write(f"üí¨ **–ß–∞—Ç —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º: {current_doc_name}**")
    st.write(f"üìä **–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {search_fragments} –∏–∑ {fragments_count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ (~{search_fragments * 2000} —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)**")
    
    question = st.chat_input("–í–∞—à –≤–æ–ø—Ä–æ—Å...")

    if question:
        with st.chat_message("user"):
            st.write(question)
            
        with st.chat_message("assistant"):
            with st.spinner("–ò—â—É –æ—Ç–≤–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ..."):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
                related_documents = st.session_state.retriever.invoke(question)
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ retriever –≤–µ—Ä–Ω—É–ª –±–æ–ª—å—à–µ)
                related_documents = related_documents[:search_fragments]
                
                answer = answer_question(question, related_documents)
                st.write(answer)
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∏—Å–∫–µ
                with st.expander("üîç –î–µ—Ç–∞–ª–∏ –ø–æ–∏—Å–∫–∞"):
                    st.write(f"**–ù–∞–π–¥–µ–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤:** {len(related_documents)}")
                    st.write(f"**–û–±—â–∏–π –æ–±—ä–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:** ~{sum(len(doc.page_content) for doc in related_documents)} —Å–∏–º–≤–æ–ª–æ–≤")
                    
                    for i, doc in enumerate(related_documents[:3], 1):
                        st.write(f"**–§—Ä–∞–≥–º–µ–Ω—Ç {i}:** {doc.page_content[:200]}...")


